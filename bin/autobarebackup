#!/usr/bin/env bash

export PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"

DIR_BACKUP_BASE="/backup"
LOCK_DIR="/var/lock"
#EMAIL_ALERT=""


SILENT=0
PARALLEL_COMPRESS=4
INCREMENTAL_DIR_MAX=1
SCRIPT_UPDATE_URL="https://raw.githubusercontent.com/macskas/autobarebackup/master/bin/autobarebackup"

FILE_CONFIG="/etc/autobarebackup/autobarebackup.conf"

BINARY_NEED=( "find" "tar" "gzip" "mail" )
CONFIG_ALLOWED=( "SILENT" "PARALLEL_COMPRESS" "INCREMENTAL_DIR_MAX" "INCREMENTAL_DATE" "EMAIL_ALERT" "DIR_BACKUP_BASE" "SCRIPT_UPDATE_URL" "LOCK_DIR" )
MYHOST=$(hostname)
DATE_SUFFIX=$(date +%Y-%m-%d_%Hh%Mm_%a)
INCREMENTAL_DATE=$(date --date="last sunday" +%Y-%m-%d)

# will be overridden once config is read
LOGFILE="$DIR_BACKUP_BASE/backup.$DATE_SUFFIX.log"
COMPRESS_PROGRAM="gzip"

UNIQ_FILENAME="sync.lock"

SCRIPT_VERSION="2016022501"


generate_uniq_filename()
{
    UNIQ_FILENAME=$(echo "$0"|md5sum 2>&1|awk '{print $1}')
    if [ "${#UNIQ_FILENAME}" -eq 0 ]; then
        UNIQ_FILENAME="sync.lock"
    else
        UNIQ_FILENAME="$UNIQ_FILENAME.lock"
    fi
}

do_help()
{
    echo "autobarebackup $SCRIPT_VERSION, basic autobareback script. For easy bacula backups."
    echo "Usage: $0 [option]"
    echo -e "\t-h: show help"
    echo -e "\t-q: silent mode"
    echo -e "\t-u: update from github (master)"
    echo -e "\t-c <config>: read a config file"
    exit 0
}


do_getopt()
{
    local opt=""
    while getopts "c:hqu" opt; do
	case "$opt" in
	    c)
            FILE_CONFIG="$OPTARG"
        ;;
	    h)
            do_help
        ;;
	    q)
            SILENT=1
        ;;
	    u)
            do_update
            exit 0
        ;;
	    \?)
            echo "Option -$OPTARG invalid" >&2
            do_help
	    ;;
	esac
    done
}

do_read_config()
{
    if [ ! -f "$FILE_CONFIG" ]; then
	    return 0
    fi
    local i=0
    local e=${#CONFIG_ALLOWED[*]}
    local implode_string=""
    for (( i=0; i<e; ++i )); do
        if [ $i -eq 0 ]; then
            implode_string=${CONFIG_ALLOWED[$i]}
        else
            implode_string="$implode_string|${CONFIG_ALLOWED[$i]}"
        fi
    done
    source <(grep -E "^\s*($implode_string)=" $FILE_CONFIG)
}

do_error()
{
    local ftime=$(date +%Y-%m-%d" "%H:%M:%S)
    local msg="$ftime ERROR > $@"

    echo "$msg" >> $LOGFILE
    echo "$msg" 1>&2
    if [ ${#EMAIL_ALERT} -gt 5 ];then
        cat "$LOGFILE"|mail -s "Backup Failed. ($MYHOST - $msg)" "$EMAIL_ALERT"
    fi
    exit 1
}

do_info()
{
    local ftime=$(date +%Y-%m-%d" "%H:%M:%S)
    local category="$1"
    shift
    local msg="$ftime INFO $category > $@"
    echo "$msg" >> $LOGFILE
    if [ $SILENT -eq 1 ]; then
        return 0
    fi
    echo "$msg"
}

do_check_binary()
{
    local e=${#BINARY_NEED[*]}
    local i=0
    for (( i=0; i<e; i++ )); do
        do_check_binary_one "${BINARY_NEED[$i]}"
        if [ $? -ne 0 ]; then
            do_error "Missing binary: ${BINARY_NEED[$i]}"
        fi
    done
    if [ $PARALLEL_COMPRESS -gt 1 ]; then
        do_check_binary_one pigz
        if [ $? -eq 0 ]; then
            COMPRESS_PROGRAM="pigz -p$PARALLEL_COMPRESS"
        fi
    fi
}

do_check_binary_one()
{
    local prg="$1"
    which "$prg" >/dev/null
    if [ $? -ne 0 ]; then
        return 1
    fi
    return 0
}

do_update()
{
    local curl_ok=0
    local wget_ok=0
    local tmp_f=""
    do_check_binary_one "curl" && curl_ok=1
    do_check_binary_one "wget" && wget_ok=1
    do_check_binary_one "mktemp" || do_error "Need mktemp command for updating"
    do_info "do_update" "download new version from $SCRIPT_URL"
    if [ "$curl_ok" -eq 1 ]; then
        tmp_f=$(mktemp)
        [ -f "$tmp_f" ] || do_error "mktemp failed."
        curl -s --connect-timeout 10 -o "$tmp_f" "$SCRIPT_UPDATE_URL"
        if [ $? -ne 0 ]; then
            rm -f "$tmp_f"
            do_error "Unable to download new version"
        fi
    elif [ "$wget_ok" -eq 1 ]; then
        tmp_f=$(mktemp)
        [ -f "$tmp_f" ] || do_error "mktemp failed."
        wget -q -T 10 -O "$tmp_f" "$SCRIPT_UPDATE_URL"
        if [ $? -ne 0 ]; then
            rm -f "$tmp_f"
            do_error "Unable to download new version"
        fi
    else
        do_error "Need wget or curl"
    fi
    local tmp_version=$(grep -E '^\s*SCRIPT_VERSION="[0-9]+"' "$tmp_f"|grep -Eo "[0-9]+")
    if [ ${#tmp_version} -lt 5 ]; then
        rm -f "$tmp_f"
        do_error "Unable to find script_version in the new script."
    fi
    if [ "$tmp_version" -le "$SCRIPT_VERSION" ]; then
        do_info "do_update" "You have the latest version. No update required."
        rm -f "$tmp_f"
        exit 0
    fi
    do_info "do_update" "Update is required - new version: $tmp_version."
    # sanity check on arg0
    grep -q SCRIPT_VERSION "$0"
    if [ $? -ne 0 ]; then
        rm -f "$tmp_f"
        do_error "Something wrong with argv[0]: ($0)"
    fi
    chmod 755 "$tmp_f" || do_error "Chmod failed on $tmp_f."
    mv "$tmp_f" "$0"
    do_info "do_update" "Updated was successful."
    exit 0
}

do_cleanup_maxfiles()
{
    local dir_incr="$1"
    local offset="$2"
    offset=$(( offset+0 ))
    local fp=""
    if [ "$INCREMENTAL_DIR_MAX" -le 0 ]; then
        do_info "do_cleanup_maxfiles" "Skip maxfiles cleanup"
        return 0
    fi
    if [ ! -d "$dir_incr" ]; then
    	do_info "do_cleanup_maxfiles" "Skip cleanup - First run"
    	return 0
    fi
    echo "$dir_incr"|grep -q "$DIR_BACKUP_BASE"
    if [ $? -ne 0 ]; then
        do_error "Something wrong with do_cleanup_maxfiles first parameter($dir_incr)"
    fi
    local numfiles=$(find "$dir_incr/" -type d -printf "%P\n"|grep -v '^$'|wc -l)
    local real_maxfiles=$(( INCREMENTAL_DIR_MAX-offset ))
    if [ $numfiles -gt $real_maxfiles ]; then
        local need_files=$(( numfiles-real_maxfiles ))
        if [ $need_files -gt 0 ]; then
            while read fp; do
                do_info "do_cleanup_maxfiles" "Delete backup $fp"
                # just to be sure its not rm -rf / :DDD even if it is not possible here
                if [ ${#fp} -lt 7 ]; then
            	    do_error "Could not delete $fp."
                fi
        	rm -rf "$fp"
            done < <(find "$dir_incr/" -type d -printf "%T@ %h/%f\n"|sort -n|cut -d ' ' -f2-|head -n "$need_files")
        fi
    else
        do_info "do_cleanup_maxfiles" "No action required. Files found: $numfiles"
        return 0
    fi
}

do_squashfs_backup()
{
    local namespace_fullpath="$1"
    local include_file="$2"
    local exclude_file="$3"

    local FULL_DATE=$(date +%Y-%m-%d)
    local dir_base_full="$namespace_fullpath/full/$FULL_DATE"
    local dir_latest="$namespace_fullpath/latest"
    local link_latest="$dir_latest/$FULL_DATE"

    do_cleanup_maxfiles "$namespace_fullpath/full" 0

    if [ ! -d "$dir_base_full" ]; then
	mkdir -p "$dir_base_full" || do_error "Failed to create directory: $dir_base_full"
    fi

    # mksquashfs /www backup_file.fs -ef /backup/www/backup.exclude -wildcards -keep-as-directory
    do_info "do_squashfs_backup" "Running mksquashfs"
    local tmp_include=""
    local file_backup=""
    local tmp_include_formatted=""
    while read tmp_include; do
	if [ ! -e "$tmp_include" ]; then
	    do_error "Missing backup target: $tmp_include (from $include_file)"
	fi
	tmp_include_formatted=$(echo "$tmp_include"|sed 's,[^a-zA-Z0-9_-],_,g')
	file_backup="$dir_base_full/${DATE_SUFFIX}-${tmp_include_formatted}.squashfs"
	if [ -f "$file_backup" ]; then
	    do_error "File already exists: $file_backup"
	fi
        if [ -f "$exclude_file"  ]; then
    	    do_info "do_squashfs_backup" "mksquashfs $tmp_include $file_backup -ef $exclude_file -no-progress -wildcards -noappend -keep-as-directory"
	    mksquashfs "$tmp_include" "$file_backup" -ef "$exclude_file" -no-progress -wildcards -noappend -keep-as-directory >>$LOGFILE 2>>$LOGFILE
	else
	    do_info "do_squashfs_backup" "mksquashfs $tmp_include $file_backup -no-progress -wildcards -noappend -keep-as-directory"
	    mksquashfs "$tmp_include" "$file_backup" -no-progress -wildcards -noappend -keep-as-directory >>$LOGFILE 2>>$LOGFILE
	fi
	if [ $? -ne 0 ]; then
    	    do_error "mksquashfs failed";
	else
    	    do_info "do_squashfs_backup" "Finished."
	fi
    done < "$include_file"

    if [ ! -d "$dir_latest" ]; then
	mkdir -p "$dir_latest" || do_error "Failed to create directory: $dir_latest"
    fi

    find "$dir_latest/" -type l -delete
    ln -s "$dir_base_full" "$link_latest" || do_error "Failed to create symlink: $dir_base_full -> $link_latest"
    do_cleanup_maxfiles "$namespace_fullpath/full" 0

}

do_compressed_backup()
{
    local namespace_fullpath="$1"
    local include_file="$2"
    local exclude_file="$3"

    local dir_base_inc="$namespace_fullpath/incr/$INCREMENTAL_DATE"
    local file_snar="$namespace_fullpath/incr/$INCREMENTAL_DATE/files.snar"
    local file_backup="$namespace_fullpath/incr/$INCREMENTAL_DATE/$DATE_SUFFIX.tar.gz"
    local dir_latest="$namespace_fullpath/latest"
    local link_latest="$dir_latest/$INCREMENTAL_DATE"

    do_cleanup_maxfiles "$namespace_fullpath/incr" 0
    if [ ! -d "$dir_base_inc" ]; then
        mkdir -p "$dir_base_inc" || do_error "Failed to create directory: $dir_base_inc"
    fi

    if [ -f "$file_backup" ]; then
        do_error "File already exists: $file_backup"
    fi

    do_info "do_compressed_backup" "Running compress"
    if [ -f "$exclude_file"  ]; then
        tar -T "$include_file" --exclude-from="$exclude_file" -g$file_snar -cf - 2>>$LOGFILE | $COMPRESS_PROGRAM >$file_backup 2>>$LOGFILE
    else
        tar -T "$include_file" -g$file_snar -cf - 2>>$LOGFILE | $COMPRESS_PROGRAM >$file_backup 2>>$LOGFILE
    fi
    if [ $? -ne 0 ]; then
        do_error "Compress failed";
    else
        do_info "do_compressed_backup" "Finished."
    fi

    if [ ! -d "$dir_latest" ]; then
        mkdir -p "$dir_latest" || do_error "Failed to create directory: $dir_latest"
    fi

    find "$dir_latest/" -type l -delete
    ln -s "$dir_base_inc" "$link_latest" || do_error "Failed to create symlink: $dir_base_inc -> $link_latest"
    do_cleanup_maxfiles "$namespace_fullpath/incr" 0
}

do_backup()
{
    local backup_namespace=""
    local namespace_fullpath=""
    local include_file=""
    local exclude_file=""
    local method_file=""
    local method_local="tar"

    if [ ! -d "$DIR_BACKUP_BASE" ]; then
        do_error "Missing base backup directory: $DIR_BACKUP_BASE"
    fi

    while read backup_namespace; do
        if [ ${#backup_namespace} -eq 0 ]; then
            continue
        fi

        method_local="tar"
        namespace_fullpath="$DIR_BACKUP_BASE/$backup_namespace"
        include_file="$namespace_fullpath/backup.include"
        do_info "include file: $include_file"
        exclude_file="$namespace_fullpath/backup.exclude"
        method_file="$namespace_fullpath/backup.method"
        if [ ! -f "$include_file" ]; then
            do_info "do_backup" "Skip backup - $backup_namespace - missing include file: $include_file"
            continue
        fi
        if [ -f "$method_file" ]; then
            method_local=$(head -n1 $method_file)
            if [ "$method_local" = "tar" ]; then
                method_local="tar"
            elif [ "$method_local" = "mksquashfs" ]; then
                do_check_binary_one "$method_local" || do_error "mksquashfs command is unavailable"
            else
                do_error "Unknown method in file: $method_file"
            fi
        fi
        do_info "do_backup" "namespace: $namespace_fullpath"
        case "$method_local" in
            mksquashfs)
                do_squashfs_backup "$namespace_fullpath" "$include_file" "$exclude_file"
            ;;
            *)
                do_compressed_backup "$namespace_fullpath" "$include_file" "$exclude_file"
        esac
    done < <(find "$DIR_BACKUP_BASE" -maxdepth 1 -type d -printf "%P\n")
}

do_cleanup_logs()
{
    find "$DIR_BACKUP_BASE/" -type f -name "*.log" -mtime +7 -delete
}

do_getopt "$@"
do_read_config
LOGFILE="$DIR_BACKUP_BASE/backup.$DATE_SUFFIX.log"
do_check_binary
generate_uniq_filename

(
    flock -n -e 200 || do_error "Another backup is already running"
    do_backup
    do_cleanup_logs
) 200>$LOCK_DIR/$UNIQ_FILENAME
